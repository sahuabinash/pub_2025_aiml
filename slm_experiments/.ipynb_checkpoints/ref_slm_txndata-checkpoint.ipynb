{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6403c9f-f31c-449a-a0d6-2645d73ef3f1",
   "metadata": {},
   "source": [
    "### Goals\n",
    "1. Generate sequences from transaction data\n",
    "2. Create random masks for MLM\n",
    "3. Create Dataloader\n",
    "4. Encoder style transformer\n",
    "5. Training Loop\n",
    "6. Downstream adaptation for other tasks\n",
    "\n",
    "### Note that it has mixed examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c737d-4848-47c5-b3de-0d8f1e89e50b",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e037060-e7ba-4ee8-9471-45b8d39f2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s say your DataFrame looks like this:\n",
    "df = pd.DataFrame({\n",
    "    \"user_id\": [...],\n",
    "    \"timestamp\": [...],  # Ensure it's sorted\n",
    "    \"merchant_id\": [...],\n",
    "    \"txn_type\": [...],\n",
    "    \"country\": [...],\n",
    "    \"amount\": [...]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97423bbb-ccd8-4055-8bf3-193f07124ac4",
   "metadata": {},
   "source": [
    "1. Group by user_id\n",
    "2. Sort by timestamp\n",
    "3. Chunk into sequences of max length 12\n",
    "4. Pad shorter sequences with -999\n",
    "5. Optionally mark positions for -100 as mask target (e.g., in MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfd124-f396-4b0c-89fa-35e55a78713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SEQ_LEN = 12\n",
    "PAD_TOKEN = -999\n",
    "MASK_TOKEN = -100  # if needed for MLM\n",
    "\n",
    "def create_sequences(df, categorical_cols, numerical_cols):\n",
    "    sequences = []\n",
    "\n",
    "    # Sort data\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "\n",
    "    for user_id, user_df in df.groupby(\"user_id\"):\n",
    "        n = len(user_df)\n",
    "        for i in range(0, n, SEQ_LEN):\n",
    "            chunk = user_df.iloc[i:i+SEQ_LEN]\n",
    "\n",
    "            seq = {}\n",
    "            for col in categorical_cols + numerical_cols:\n",
    "                values = chunk[col].tolist()\n",
    "                # Pad if shorter than SEQ_LEN\n",
    "                if len(values) < SEQ_LEN:\n",
    "                    pad_value = PAD_TOKEN if col in categorical_cols + numerical_cols else 0\n",
    "                    values += [pad_value] * (SEQ_LEN - len(values))\n",
    "                seq[col] = values[:SEQ_LEN]  # ensure it's exactly SEQ_LEN\n",
    "\n",
    "            sequences.append(seq)\n",
    "\n",
    "    return pd.DataFrame(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977cbea-58a2-4179-9339-c63572a85493",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"merchant_id\", \"txn_type\", \"country\"]\n",
    "numerical_cols = [\"amount\"]\n",
    "\n",
    "seq_df = create_sequences(df, categorical_cols, numerical_cols)\n",
    "\n",
    "# You get a DataFrame where each row = one sequence of 12 txns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174b0e56-d071-465b-b99c-5e761944cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Tensor Dataset\n",
    "import torch\n",
    "\n",
    "def to_tensor_dataset(seq_df, categorical_cols, numerical_cols):\n",
    "    tensors = {}\n",
    "    for col in categorical_cols:\n",
    "        tensors[col] = torch.tensor(seq_df[col].tolist(), dtype=torch.long)\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        tensors[col] = torch.tensor(seq_df[col].tolist(), dtype=torch.float)\n",
    "\n",
    "    return tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab9645-5605-46d0-a7bd-bc03a5b09c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Add Masked Positions for MLM\n",
    "def add_masking(input_tensor, mask_prob=0.15):\n",
    "    \"\"\"\n",
    "    Replace random tokens with MASK_TOKEN (-100) for supervised learning\n",
    "    \"\"\"\n",
    "    mask = torch.rand_like(input_tensor.float()) < mask_prob\n",
    "    labels = input_tensor.clone()\n",
    "    labels[~mask] = -100  # no loss on unmasked tokens\n",
    "    input_tensor[mask] = MASK_TOKEN\n",
    "    return input_tensor, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4307cb3-6869-46b8-aec6-eb558d5a4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final output looks like this\n",
    "{\n",
    "    \"merchant_id\": [12, 45, 3, ..., -999, -999],\n",
    "    \"txn_type\":    [1, 0, 2, ..., -999, -999],\n",
    "    \"country\":     [5, 5, 3, ..., -999, -999],\n",
    "    \"amount\":      [100.0, 59.0, 22.5, ..., -999.0, -999.0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786a1864-a26e-4470-95ea-70c1bd0b7ef7",
   "metadata": {},
   "source": [
    "### Create Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fba51-af58-4f39-8534-65c54f69df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Convert Sample Data to Tensor Dataset\n",
    "categorical_cols = [\"merchant_id\", \"txn_type\", \"country\"]\n",
    "numerical_cols = [\"amount\"]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TxnDataset(Dataset):\n",
    "    def __init__(self, df, categorical_cols, numerical_cols, mask_prob=0.15):\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.mask_prob = mask_prob\n",
    "        self.inputs = {}\n",
    "        self.labels = {}\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            t = torch.tensor(df[col].tolist(), dtype=torch.long)\n",
    "            masked, labels = self.add_masking(t)\n",
    "            self.inputs[col] = masked\n",
    "            self.labels[col] = labels\n",
    "\n",
    "        for col in numerical_cols:\n",
    "            self.inputs[col] = torch.tensor(df[col].tolist(), dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.inputs.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            {k: v[idx] for k, v in self.inputs.items()},\n",
    "            {k: self.labels[k][idx] for k in self.labels}\n",
    "        )\n",
    "\n",
    "    def add_masking(self, x):\n",
    "        prob = torch.rand_like(x.float()) < self.mask_prob\n",
    "        labels = x.clone()\n",
    "        labels[~prob] = -100  # no loss where not masked\n",
    "        x = x.masked_fill(prob, -100)\n",
    "        return x, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e237d5d-077b-4aa2-8854-0cfd99826a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Create DataLoader\n",
    "dataset = TxnDataset(seq_df, categorical_cols, numerical_cols)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b05f0-3917-4112-91d3-a09b0da346a6",
   "metadata": {},
   "source": [
    "### Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4497e7-f2a9-40d1-8c47-ed8f33364168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Simulate categorical fields (merchant_id, txn_type, country)\n",
    "num_merchants = 500\n",
    "num_txn_types = 5\n",
    "num_countries = 20\n",
    "\n",
    "# Simulate numeric field (amount in bucketed form)\n",
    "num_amount_buckets = 100\n",
    "\n",
    "# Create 10k users, 128 txns each\n",
    "num_sequences = 10000\n",
    "seq_len = 128\n",
    "\n",
    "def simulate_transaction_sequence():\n",
    "    return {\n",
    "        'merchant_id': np.random.randint(0, num_merchants, seq_len),\n",
    "        'txn_type': np.random.randint(0, num_txn_types, seq_len),\n",
    "        'country': np.random.randint(0, num_countries, seq_len),\n",
    "        'amount_bucket': np.random.randint(0, num_amount_buckets, seq_len)\n",
    "    }\n",
    "\n",
    "data = [simulate_transaction_sequence() for _ in range(num_sequences)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ed4bb-ce42-4263-8fc5-33ccb567ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclass with masking\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MASK_TOKEN = -1  # Special ID to represent [MASK]\n",
    "\n",
    "class BankTxnDataset(Dataset):\n",
    "    def __init__(self, data, mask_prob=0.15):\n",
    "        self.data = data\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        inputs = {}\n",
    "        labels = {}\n",
    "\n",
    "        for field, values in item.items():\n",
    "            values = np.array(values)\n",
    "            mask = np.random.rand(len(values)) < self.mask_prob\n",
    "\n",
    "            labels[field] = np.where(mask, values, -100)  # For loss calc\n",
    "            inputs[field] = np.where(mask, MASK_TOKEN, values)\n",
    "\n",
    "        return {k: torch.tensor(v, dtype=torch.long) for k, v in inputs.items()}, \\\n",
    "               {k: torch.tensor(v, dtype=torch.long) for k, v in labels.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2cb11-6529-4801-bf40-ef09aa652976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b22cf579-3141-4f7c-8741-3ef8a825f2c9",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f206e-6621-4891-ad06-94a445c0374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransactionBERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            'merchant_id': nn.Embedding(config['num_merchants'] + 2, config['d_model']),\n",
    "            'txn_type': nn.Embedding(config['num_txn_types'] + 2, config['d_model']),\n",
    "            'country': nn.Embedding(config['num_countries'] + 2, config['d_model']),\n",
    "            'amount_bucket': nn.Embedding(config['num_amount_buckets'] + 2, config['d_model']),\n",
    "        })\n",
    "\n",
    "        self.pos_emb = nn.Embedding(config['max_seq_len'], config['d_model'])\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=config['d_model'],\n",
    "                nhead=config['nhead'],\n",
    "                dim_feedforward=config['ff_dim'],\n",
    "                dropout=0.1\n",
    "            ),\n",
    "            num_layers=config['num_layers']\n",
    "        )\n",
    "\n",
    "        self.output_heads = nn.ModuleDict({\n",
    "            k: nn.Linear(config['d_model'], config[f'num_{k}s'] + 2)\n",
    "            for k in self.embeddings.keys()\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = next(iter(x.values())).shape\n",
    "\n",
    "        # Positional encoding\n",
    "        pos = torch.arange(seq_len, device=x['merchant_id'].device).unsqueeze(0)\n",
    "        pos_emb = self.pos_emb(pos)\n",
    "\n",
    "        # Sum field-wise embeddings\n",
    "        x_emb = sum([self.embeddings[k](v) for k, v in x.items()]) + pos_emb\n",
    "\n",
    "        # Transformer encoding\n",
    "        encoded = self.encoder(x_emb)\n",
    "\n",
    "        # Output heads for each field\n",
    "        outputs = {k: head(encoded) for k, head in self.output_heads.items()}\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851a8887-fcd5-4e13-9c11-e7fe168c7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Loop\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, device, epochs=3):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, labels in tqdm(dataloader):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = {k: v.to(device) for k, v in labels.items()}\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = sum([\n",
    "                F.cross_entropy(\n",
    "                    outputs[k].view(-1, outputs[k].size(-1)),\n",
    "                    labels[k].view(-1),\n",
    "                    ignore_index=-100\n",
    "                )\n",
    "                for k in outputs\n",
    "            ])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e2e62-6ac7-45eb-b052-f241a663a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Setup\n",
    "config = {\n",
    "    'd_model': 64,\n",
    "    'nhead': 4,\n",
    "    'ff_dim': 128,\n",
    "    'num_layers': 3,\n",
    "    'max_seq_len': 128,\n",
    "    'num_merchants': num_merchants,\n",
    "    'num_txn_types': num_txn_types,\n",
    "    'num_countries': num_countries,\n",
    "    'num_amount_buckets': num_amount_buckets\n",
    "}\n",
    "\n",
    "dataset = BankTxnDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = TransactionBERT(config)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataloader, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714bfc9-140d-44a6-816d-c93139e5ce3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6ed5ea-c809-4b9b-9d6d-383b894e220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Instantiate Model\n",
    "config = {\n",
    "    \"d_model\": 64,\n",
    "    \"num_heads\": 4,\n",
    "    \"num_layers\": 2,\n",
    "    \"max_seq_len\": 12,\n",
    "    \"vocab_size_merchant\": 200,\n",
    "    \"num_types\": 10,\n",
    "    \"num_countries\": 20\n",
    "}\n",
    "model = TransactionBERT(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c9d3c-d20f-4c94-b1cb-d1df70c08386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Training Loop\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)  # outputs is a dict\n",
    "\n",
    "        loss = 0.0\n",
    "        for field in batch_y:\n",
    "            out = outputs[field].reshape(-1, outputs[field].shape[-1])\n",
    "            tgt = batch_y[field].reshape(-1)\n",
    "            loss += criterion(out, tgt)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cf2ee0-28c2-4599-988b-4e87aa3f18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Inspect Output\n",
    "model.eval()\n",
    "sample = next(iter(dataloader))[0]\n",
    "with torch.no_grad():\n",
    "    output = model(sample)\n",
    "\n",
    "print(\"Predicted merchant_id:\", output[\"merchant_id\"].argmax(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f58ea2-6372-4b8c-bd17-691ad4657a1a",
   "metadata": {},
   "source": [
    "* Make sure any unknown or padded tokens in your vocab are mapped to index 0 or PAD_ID consistently.\n",
    "\n",
    "* Masked tokens should use -100 only for label masking. Inputs can use a different index (e.g., MASK_ID = vocab_size + 1) if you want to train a true MLM like BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbff0f3-31ee-4e0e-8b70-22755c8d57b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff2959-532a-4b97-ab30-9362b3ff374c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6d584-5ead-4c79-b233-26ac0147c266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549896ea-fb2a-43ab-a734-e7fa4d299030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c405f0-ee6d-4600-9bb0-490a09fc1ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4f092-4e0c-4c5b-b2b4-77119b65a4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightpynlp",
   "language": "python",
   "name": "lightpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
